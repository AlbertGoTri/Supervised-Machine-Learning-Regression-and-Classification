{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f58d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Handwritten parser of dependency specifiers.\n",
    "\n",
    "The docstring for each __parse_* function contains EBNF-inspired grammar representing\n",
    "the implementation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e16d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd9b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from typing import NamedTuple, Sequence, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c600be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ._tokenizer import DEFAULT_RULES, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, value: str) -> None:\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.value\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"<{self.__class__.__name__}('{self}')>\"\n",
    "\n",
    "    def serialize(self) -> str:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb15f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable(Node):\n",
    "    def serialize(self) -> str:\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56983cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(Node):\n",
    "    def serialize(self) -> str:\n",
    "        return f'\"{self}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f4458",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Op(Node):\n",
    "    def serialize(self) -> str:\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293e0653",
   "metadata": {},
   "outputs": [],
   "source": [
    "MarkerVar = Union[Variable, Value]\n",
    "MarkerItem = Tuple[MarkerVar, Op, MarkerVar]\n",
    "MarkerAtom = Union[MarkerItem, Sequence[\"MarkerAtom\"]]\n",
    "MarkerList = Sequence[Union[\"MarkerList\", MarkerAtom, str]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf3606",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParsedRequirement(NamedTuple):\n",
    "    name: str\n",
    "    url: str\n",
    "    extras: list[str]\n",
    "    specifier: str\n",
    "    marker: MarkerList | None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12593554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------\n",
    "# Recursive descent parser for dependency specifier\n",
    "# --------------------------------------------------------------------------------------\n",
    "def parse_requirement(source: str) -> ParsedRequirement:\n",
    "    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cfc5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:\n",
    "    \"\"\"\n",
    "    requirement = WS? IDENTIFIER WS? extras WS? requirement_details\n",
    "    \"\"\"\n",
    "    tokenizer.consume(\"WS\")\n",
    "\n",
    "    name_token = tokenizer.expect(\n",
    "        \"IDENTIFIER\", expected=\"package name at the start of dependency specifier\"\n",
    "    )\n",
    "    name = name_token.text\n",
    "    tokenizer.consume(\"WS\")\n",
    "\n",
    "    extras = _parse_extras(tokenizer)\n",
    "    tokenizer.consume(\"WS\")\n",
    "\n",
    "    url, specifier, marker = _parse_requirement_details(tokenizer)\n",
    "    tokenizer.expect(\"END\", expected=\"end of dependency specifier\")\n",
    "\n",
    "    return ParsedRequirement(name, url, extras, specifier, marker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60c6c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_requirement_details(\n",
    "    tokenizer: Tokenizer,\n",
    ") -> tuple[str, str, MarkerList | None]:\n",
    "    \"\"\"\n",
    "    requirement_details = AT URL (WS requirement_marker?)?\n",
    "                        | specifier WS? (requirement_marker)?\n",
    "    \"\"\"\n",
    "\n",
    "    specifier = \"\"\n",
    "    url = \"\"\n",
    "    marker = None\n",
    "\n",
    "    if tokenizer.check(\"AT\"):\n",
    "        tokenizer.read()\n",
    "        tokenizer.consume(\"WS\")\n",
    "\n",
    "        url_start = tokenizer.position\n",
    "        url = tokenizer.expect(\"URL\", expected=\"URL after @\").text\n",
    "        if tokenizer.check(\"END\", peek=True):\n",
    "            return (url, specifier, marker)\n",
    "\n",
    "        tokenizer.expect(\"WS\", expected=\"whitespace after URL\")\n",
    "\n",
    "        # The input might end after whitespace.\n",
    "        if tokenizer.check(\"END\", peek=True):\n",
    "            return (url, specifier, marker)\n",
    "\n",
    "        marker = _parse_requirement_marker(\n",
    "            tokenizer, span_start=url_start, after=\"URL and whitespace\"\n",
    "        )\n",
    "    else:\n",
    "        specifier_start = tokenizer.position\n",
    "        specifier = _parse_specifier(tokenizer)\n",
    "        tokenizer.consume(\"WS\")\n",
    "\n",
    "        if tokenizer.check(\"END\", peek=True):\n",
    "            return (url, specifier, marker)\n",
    "\n",
    "        marker = _parse_requirement_marker(\n",
    "            tokenizer,\n",
    "            span_start=specifier_start,\n",
    "            after=(\n",
    "                \"version specifier\"\n",
    "                if specifier\n",
    "                else \"name and no valid version specifier\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return (url, specifier, marker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398fc8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_requirement_marker(\n",
    "    tokenizer: Tokenizer, *, span_start: int, after: str\n",
    ") -> MarkerList:\n",
    "    \"\"\"\n",
    "    requirement_marker = SEMICOLON marker WS?\n",
    "    \"\"\"\n",
    "\n",
    "    if not tokenizer.check(\"SEMICOLON\"):\n",
    "        tokenizer.raise_syntax_error(\n",
    "            f\"Expected end or semicolon (after {after})\",\n",
    "            span_start=span_start,\n",
    "        )\n",
    "    tokenizer.read()\n",
    "\n",
    "    marker = _parse_marker(tokenizer)\n",
    "    tokenizer.consume(\"WS\")\n",
    "\n",
    "    return marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755eb723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_extras(tokenizer: Tokenizer) -> list[str]:\n",
    "    \"\"\"\n",
    "    extras = (LEFT_BRACKET wsp* extras_list? wsp* RIGHT_BRACKET)?\n",
    "    \"\"\"\n",
    "    if not tokenizer.check(\"LEFT_BRACKET\", peek=True):\n",
    "        return []\n",
    "\n",
    "    with tokenizer.enclosing_tokens(\n",
    "        \"LEFT_BRACKET\",\n",
    "        \"RIGHT_BRACKET\",\n",
    "        around=\"extras\",\n",
    "    ):\n",
    "        tokenizer.consume(\"WS\")\n",
    "        extras = _parse_extras_list(tokenizer)\n",
    "        tokenizer.consume(\"WS\")\n",
    "\n",
    "    return extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_extras_list(tokenizer: Tokenizer) -> list[str]:\n",
    "    \"\"\"\n",
    "    extras_list = identifier (wsp* ',' wsp* identifier)*\n",
    "    \"\"\"\n",
    "    extras: list[str] = []\n",
    "\n",
    "    if not tokenizer.check(\"IDENTIFIER\"):\n",
    "        return extras\n",
    "\n",
    "    extras.append(tokenizer.read().text)\n",
    "\n",
    "    while True:\n",
    "        tokenizer.consume(\"WS\")\n",
    "        if tokenizer.check(\"IDENTIFIER\", peek=True):\n",
    "            tokenizer.raise_syntax_error(\"Expected comma between extra names\")\n",
    "        elif not tokenizer.check(\"COMMA\"):\n",
    "            break\n",
    "\n",
    "        tokenizer.read()\n",
    "        tokenizer.consume(\"WS\")\n",
    "\n",
    "        extra_token = tokenizer.expect(\"IDENTIFIER\", expected=\"extra name after comma\")\n",
    "        extras.append(extra_token.text)\n",
    "\n",
    "    return extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc41700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_specifier(tokenizer: Tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    specifier = LEFT_PARENTHESIS WS? version_many WS? RIGHT_PARENTHESIS\n",
    "              | WS? version_many WS?\n",
    "    \"\"\"\n",
    "    with tokenizer.enclosing_tokens(\n",
    "        \"LEFT_PARENTHESIS\",\n",
    "        \"RIGHT_PARENTHESIS\",\n",
    "        around=\"version specifier\",\n",
    "    ):\n",
    "        tokenizer.consume(\"WS\")\n",
    "        parsed_specifiers = _parse_version_many(tokenizer)\n",
    "        tokenizer.consume(\"WS\")\n",
    "\n",
    "    return parsed_specifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a7ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_version_many(tokenizer: Tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    version_many = (SPECIFIER (WS? COMMA WS? SPECIFIER)*)?\n",
    "    \"\"\"\n",
    "    parsed_specifiers = \"\"\n",
    "    while tokenizer.check(\"SPECIFIER\"):\n",
    "        span_start = tokenizer.position\n",
    "        parsed_specifiers += tokenizer.read().text\n",
    "        if tokenizer.check(\"VERSION_PREFIX_TRAIL\", peek=True):\n",
    "            tokenizer.raise_syntax_error(\n",
    "                \".* suffix can only be used with `==` or `!=` operators\",\n",
    "                span_start=span_start,\n",
    "                span_end=tokenizer.position + 1,\n",
    "            )\n",
    "        if tokenizer.check(\"VERSION_LOCAL_LABEL_TRAIL\", peek=True):\n",
    "            tokenizer.raise_syntax_error(\n",
    "                \"Local version label can only be used with `==` or `!=` operators\",\n",
    "                span_start=span_start,\n",
    "                span_end=tokenizer.position,\n",
    "            )\n",
    "        tokenizer.consume(\"WS\")\n",
    "        if not tokenizer.check(\"COMMA\"):\n",
    "            break\n",
    "        parsed_specifiers += tokenizer.read().text\n",
    "        tokenizer.consume(\"WS\")\n",
    "\n",
    "    return parsed_specifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------\n",
    "# Recursive descent parser for marker expression\n",
    "# --------------------------------------------------------------------------------------\n",
    "def parse_marker(source: str) -> MarkerList:\n",
    "    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59df38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:\n",
    "    retval = _parse_marker(tokenizer)\n",
    "    tokenizer.expect(\"END\", expected=\"end of marker expression\")\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d60e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_marker(tokenizer: Tokenizer) -> MarkerList:\n",
    "    \"\"\"\n",
    "    marker = marker_atom (BOOLOP marker_atom)+\n",
    "    \"\"\"\n",
    "    expression = [_parse_marker_atom(tokenizer)]\n",
    "    while tokenizer.check(\"BOOLOP\"):\n",
    "        token = tokenizer.read()\n",
    "        expr_right = _parse_marker_atom(tokenizer)\n",
    "        expression.extend((token.text, expr_right))\n",
    "    return expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1266fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:\n",
    "    \"\"\"\n",
    "    marker_atom = WS? LEFT_PARENTHESIS WS? marker WS? RIGHT_PARENTHESIS WS?\n",
    "                | WS? marker_item WS?\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer.consume(\"WS\")\n",
    "    if tokenizer.check(\"LEFT_PARENTHESIS\", peek=True):\n",
    "        with tokenizer.enclosing_tokens(\n",
    "            \"LEFT_PARENTHESIS\",\n",
    "            \"RIGHT_PARENTHESIS\",\n",
    "            around=\"marker expression\",\n",
    "        ):\n",
    "            tokenizer.consume(\"WS\")\n",
    "            marker: MarkerAtom = _parse_marker(tokenizer)\n",
    "            tokenizer.consume(\"WS\")\n",
    "    else:\n",
    "        marker = _parse_marker_item(tokenizer)\n",
    "    tokenizer.consume(\"WS\")\n",
    "    return marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72de5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:\n",
    "    \"\"\"\n",
    "    marker_item = WS? marker_var WS? marker_op WS? marker_var WS?\n",
    "    \"\"\"\n",
    "    tokenizer.consume(\"WS\")\n",
    "    marker_var_left = _parse_marker_var(tokenizer)\n",
    "    tokenizer.consume(\"WS\")\n",
    "    marker_op = _parse_marker_op(tokenizer)\n",
    "    tokenizer.consume(\"WS\")\n",
    "    marker_var_right = _parse_marker_var(tokenizer)\n",
    "    tokenizer.consume(\"WS\")\n",
    "    return (marker_var_left, marker_op, marker_var_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d527301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:\n",
    "    \"\"\"\n",
    "    marker_var = VARIABLE | QUOTED_STRING\n",
    "    \"\"\"\n",
    "    if tokenizer.check(\"VARIABLE\"):\n",
    "        return process_env_var(tokenizer.read().text.replace(\".\", \"_\"))\n",
    "    elif tokenizer.check(\"QUOTED_STRING\"):\n",
    "        return process_python_str(tokenizer.read().text)\n",
    "    else:\n",
    "        tokenizer.raise_syntax_error(\n",
    "            message=\"Expected a marker variable or quoted string\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd4c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_env_var(env_var: str) -> Variable:\n",
    "    if env_var in (\"platform_python_implementation\", \"python_implementation\"):\n",
    "        return Variable(\"platform_python_implementation\")\n",
    "    else:\n",
    "        return Variable(env_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d9e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_python_str(python_str: str) -> Value:\n",
    "    value = ast.literal_eval(python_str)\n",
    "    return Value(str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_marker_op(tokenizer: Tokenizer) -> Op:\n",
    "    \"\"\"\n",
    "    marker_op = IN | NOT IN | OP\n",
    "    \"\"\"\n",
    "    if tokenizer.check(\"IN\"):\n",
    "        tokenizer.read()\n",
    "        return Op(\"in\")\n",
    "    elif tokenizer.check(\"NOT\"):\n",
    "        tokenizer.read()\n",
    "        tokenizer.expect(\"WS\", expected=\"whitespace after 'not'\")\n",
    "        tokenizer.expect(\"IN\", expected=\"'in' after 'not'\")\n",
    "        return Op(\"not in\")\n",
    "    elif tokenizer.check(\"OP\"):\n",
    "        return Op(tokenizer.read().text)\n",
    "    else:\n",
    "        return tokenizer.raise_syntax_error(\n",
    "            \"Expected marker operator, one of <=, <, !=, ==, >=, >, ~=, ===, in, not in\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
