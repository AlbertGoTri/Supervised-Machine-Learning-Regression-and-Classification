{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d2b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aee4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterator, NoReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fade0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from .specifiers import Specifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Token:\n",
    "    name: str\n",
    "    text: str\n",
    "    position: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd0c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserSyntaxError(Exception):\n",
    "    \"\"\"The provided source text could not be parsed correctly.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        message: str,\n",
    "        *,\n",
    "        source: str,\n",
    "        span: tuple[int, int],\n",
    "    ) -> None:\n",
    "        self.span = span\n",
    "        self.message = message\n",
    "        self.source = source\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        marker = \" \" * self.span[0] + \"~\" * (self.span[1] - self.span[0]) + \"^\"\n",
    "        return \"\\n    \".join([self.message, self.source, marker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5550e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_RULES: dict[str, str | re.Pattern[str]] = {\n",
    "    \"LEFT_PARENTHESIS\": r\"\\(\",\n",
    "    \"RIGHT_PARENTHESIS\": r\"\\)\",\n",
    "    \"LEFT_BRACKET\": r\"\\[\",\n",
    "    \"RIGHT_BRACKET\": r\"\\]\",\n",
    "    \"SEMICOLON\": r\";\",\n",
    "    \"COMMA\": r\",\",\n",
    "    \"QUOTED_STRING\": re.compile(\n",
    "        r\"\"\"\n",
    "            (\n",
    "                ('[^']*')\n",
    "                |\n",
    "                (\"[^\"]*\")\n",
    "            )\n",
    "        \"\"\",\n",
    "        re.VERBOSE,\n",
    "    ),\n",
    "    \"OP\": r\"(===|==|~=|!=|<=|>=|<|>)\",\n",
    "    \"BOOLOP\": r\"\\b(or|and)\\b\",\n",
    "    \"IN\": r\"\\bin\\b\",\n",
    "    \"NOT\": r\"\\bnot\\b\",\n",
    "    \"VARIABLE\": re.compile(\n",
    "        r\"\"\"\n",
    "            \\b(\n",
    "                python_version\n",
    "                |python_full_version\n",
    "                |os[._]name\n",
    "                |sys[._]platform\n",
    "                |platform_(release|system)\n",
    "                |platform[._](version|machine|python_implementation)\n",
    "                |python_implementation\n",
    "                |implementation_(name|version)\n",
    "                |extras?\n",
    "                |dependency_groups\n",
    "            )\\b\n",
    "        \"\"\",\n",
    "        re.VERBOSE,\n",
    "    ),\n",
    "    \"SPECIFIER\": re.compile(\n",
    "        Specifier._operator_regex_str + Specifier._version_regex_str,\n",
    "        re.VERBOSE | re.IGNORECASE,\n",
    "    ),\n",
    "    \"AT\": r\"\\@\",\n",
    "    \"URL\": r\"[^ \\t]+\",\n",
    "    \"IDENTIFIER\": r\"\\b[a-zA-Z0-9][a-zA-Z0-9._-]*\\b\",\n",
    "    \"VERSION_PREFIX_TRAIL\": r\"\\.\\*\",\n",
    "    \"VERSION_LOCAL_LABEL_TRAIL\": r\"\\+[a-z0-9]+(?:[-_\\.][a-z0-9]+)*\",\n",
    "    \"WS\": r\"[ \\t]+\",\n",
    "    \"END\": r\"$\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3b1a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"Context-sensitive token parsing.\n",
    "\n",
    "    Provides methods to examine the input stream to check whether the next token\n",
    "    matches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        source: str,\n",
    "        *,\n",
    "        rules: dict[str, str | re.Pattern[str]],\n",
    "    ) -> None:\n",
    "        self.source = source\n",
    "        self.rules: dict[str, re.Pattern[str]] = {\n",
    "            name: re.compile(pattern) for name, pattern in rules.items()\n",
    "        }\n",
    "        self.next_token: Token | None = None\n",
    "        self.position = 0\n",
    "\n",
    "    def consume(self, name: str) -> None:\n",
    "        \"\"\"Move beyond provided token name, if at current position.\"\"\"\n",
    "        if self.check(name):\n",
    "            self.read()\n",
    "\n",
    "    def check(self, name: str, *, peek: bool = False) -> bool:\n",
    "        \"\"\"Check whether the next token has the provided name.\n",
    "\n",
    "        By default, if the check succeeds, the token *must* be read before\n",
    "        another check. If `peek` is set to `True`, the token is not loaded and\n",
    "        would need to be checked again.\n",
    "        \"\"\"\n",
    "        assert self.next_token is None, (\n",
    "            f\"Cannot check for {name!r}, already have {self.next_token!r}\"\n",
    "        )\n",
    "        assert name in self.rules, f\"Unknown token name: {name!r}\"\n",
    "\n",
    "        expression = self.rules[name]\n",
    "\n",
    "        match = expression.match(self.source, self.position)\n",
    "        if match is None:\n",
    "            return False\n",
    "        if not peek:\n",
    "            self.next_token = Token(name, match[0], self.position)\n",
    "        return True\n",
    "\n",
    "    def expect(self, name: str, *, expected: str) -> Token:\n",
    "        \"\"\"Expect a certain token name next, failing with a syntax error otherwise.\n",
    "\n",
    "        The token is *not* read.\n",
    "        \"\"\"\n",
    "        if not self.check(name):\n",
    "            raise self.raise_syntax_error(f\"Expected {expected}\")\n",
    "        return self.read()\n",
    "\n",
    "    def read(self) -> Token:\n",
    "        \"\"\"Consume the next token and return it.\"\"\"\n",
    "        token = self.next_token\n",
    "        assert token is not None\n",
    "\n",
    "        self.position += len(token.text)\n",
    "        self.next_token = None\n",
    "\n",
    "        return token\n",
    "\n",
    "    def raise_syntax_error(\n",
    "        self,\n",
    "        message: str,\n",
    "        *,\n",
    "        span_start: int | None = None,\n",
    "        span_end: int | None = None,\n",
    "    ) -> NoReturn:\n",
    "        \"\"\"Raise ParserSyntaxError at the given position.\"\"\"\n",
    "        span = (\n",
    "            self.position if span_start is None else span_start,\n",
    "            self.position if span_end is None else span_end,\n",
    "        )\n",
    "        raise ParserSyntaxError(\n",
    "            message,\n",
    "            source=self.source,\n",
    "            span=span,\n",
    "        )\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def enclosing_tokens(\n",
    "        self, open_token: str, close_token: str, *, around: str\n",
    "    ) -> Iterator[None]:\n",
    "        if self.check(open_token):\n",
    "            open_position = self.position\n",
    "            self.read()\n",
    "        else:\n",
    "            open_position = None\n",
    "\n",
    "        yield\n",
    "\n",
    "        if open_position is None:\n",
    "            return\n",
    "\n",
    "        if not self.check(close_token):\n",
    "            self.raise_syntax_error(\n",
    "                f\"Expected matching {close_token} for {open_token}, after {around}\",\n",
    "                span_start=open_position,\n",
    "            )\n",
    "\n",
    "        self.read()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
